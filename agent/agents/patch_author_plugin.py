from __future__ import annotations

import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict

from ..patch_author import PatchAuthor
from ..framework.agent_types import AgentResult, Stage
from ..llm.types import ChatMessage
from ..llm.service import LLMService


@dataclass
class PatchAuthorPlugin:
    id: str = "patch_author"
    stage: Stage = Stage.EDIT
    priority: int = 100

    def __post_init__(self):
        self.agent = PatchAuthor(tool_router=None, run_manager=None)

    def run(self, ctx, request=None) -> AgentResult:
        self.agent.tool_router = ctx.tool_router
        self.agent.run_manager = ctx.run_manager
        llm: LLMService = ctx.services.get("llm") if ctx.services else None
        
        ctx.events.emit(
            "patch_author.enter",
            {
                "has_services": bool(ctx.services),
                "has_llm": bool(llm),
                "provider_name": getattr(getattr(llm, "provider", None), "name", None) if llm else None,
                "model": getattr(llm, "model", None) if llm else None,
            },
        )
        
        if not llm or not llm.enabled():
            ctx.events.emit("llm.skip", {"reason": "LLM provider not configured", "code_path": "no_llm"})
            ctx.events.emit("patch_author.skip", {"reason": "no_llm", "code_path": "no_llm"})
            return AgentResult(status="skip", outputs={"notes": ["LLM æœªé…ç½®ï¼Œè·³è¿‡è‡ªåŠ¨è¡¥ä¸"]})

        ctx.events.emit(
            "patch_author.config",
            {
                "provider": getattr(llm.provider, "name", None),
                "model": getattr(llm, "model", None),
                "timeout": getattr(llm, "timeout", None),
                "base_url_set": bool(getattr(llm.provider, "base_url", None)),
            },
        )

        allowed_files = self._collect_allowed_files(ctx)
        ctx.events.emit("patch_author.allowed_files", {"count": len(allowed_files), "files": allowed_files})

        prompt_msgs = self._build_prompt(ctx, allowed_files)
        if not prompt_msgs:
            ctx.events.emit("patch_author.skip", {"reason": "empty_prompt", "code_path": "no_prompt"})
            return AgentResult(status="skip", outputs={"notes": ["æœªç”Ÿæˆ promptï¼Œè·³è¿‡è‡ªåŠ¨è¡¥ä¸"]})
        
        ctx.events.emit(
            "patch_author.prompt",
            {
                "message_count": len(prompt_msgs),
                "approx_chars": sum(len(m.content) for m in prompt_msgs),
                "files_referenced": self._extract_files_from_context(ctx),
            },
        )

        max_retries = 3
        attempt = 0
        final_edits = None
        last_error = None

        while attempt <= max_retries:
            if attempt > 0:
                ctx.events.emit("patch.retry", {"attempt": attempt, "error": last_error})

            ctx.events.emit(
                "llm.call",
                {"provider": getattr(llm.provider, "name", "unknown"), "model": llm.model, "attempt": attempt},
            )
            ctx.events.emit(
                "llm.request",
                {
                    "provider": getattr(llm.provider, "name", "unknown"),
                    "model": llm.model,
                    "timeout": llm.timeout,
                    "approx_prompt_bytes": sum(len(m.content) for m in prompt_msgs),
                },
            )
            
            resp = llm.generate_patch(prompt_msgs)
            response_text = resp.get("content", "") if isinstance(resp, dict) else ""
            
            ctx.events.emit(
                "llm.response",
                {
                    "ok": resp.get("ok"),
                    "latency_ms": resp.get("latency_ms"),
                    "response_bytes": len(response_text),
                    "error": resp.get("error"),
                },
                level="error" if not resp.get("ok") else "info",
            )

            if not resp.get("ok"):
                # Print debug info when LLM call fails
                print("\n" + "="*80)
                print("âŒ LLM è°ƒç”¨å¤±è´¥")
                print("="*80)
                print(f"é”™è¯¯: {resp.get('error')}")
                content_dbg = resp.get("content") or ""
                if content_dbg:
                    print("\nğŸ“¥ æ¨¡å‹è¿”å›å†…å®¹ (å‰ 2000 å­—ç¬¦):")
                    print(content_dbg[:2000])
                if attempt == 0:
                    print(f"\nğŸ“ Prompt (å‰ 1000 å­—ç¬¦):")
                    prompt_preview = "\n".join(m.content for m in prompt_msgs)[:1000]
                    print(prompt_preview)
                print("="*80 + "\n")
                
                ctx.events.emit("patch_author.skip", {"reason": resp.get("error") or "llm_failed", "code_path": "resp_not_ok"})
                return AgentResult(status="skip", outputs={"notes": [f"LLM ç”Ÿæˆå¤±è´¥: {resp.get('error')}"]})

            # Parse JSON edits from response
            edits, parse_error = self._parse_edits(response_text)
            if parse_error:
                # Print debug info when parsing fails
                print("\n" + "="*80)
                print("âŒ JSON è§£æå¤±è´¥")
                print("="*80)
                print(f"é”™è¯¯: {parse_error}")
                print(f"\nğŸ“¥ æ¨¡å‹è¿”å›å†…å®¹ (å‰ 2000 å­—ç¬¦):")
                print(response_text[:2000])
                print("="*80 + "\n")
                
                ctx.events.emit("patch.parse_fail", {"error": parse_error})
                if attempt < max_retries:
                    prompt_msgs.append(ChatMessage(
                        role="user",
                        content=(
                            "ä½ çš„è¾“å‡ºæ— æ³•è§£æä¸º JSONã€‚è¯·ä¸¥æ ¼è¾“å‡ºçº¯ JSON æ•°ç»„ï¼Œä¸è¦åŒ…å« ```json æˆ– ``` ä¹‹ç±»çš„ä»£ç å—æ ‡è®°ï¼Œ"
                            "ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šæˆ–é¢å¤–æ–‡æœ¬ã€‚å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªç¼–è¾‘å¯¹è±¡ã€‚"
                        ),
                    ))
                    attempt += 1
                    last_error = parse_error
                    continue
                else:
                    ctx.events.emit("patch_author.skip", {"reason": "parse_fail_after_retry", "code_path": "parse_fail"})
                    return AgentResult(status="skip", outputs={"notes": [f"æ— æ³•è§£æ LLM è¾“å‡º: {parse_error}"]})

            # Validate edits via protocol + executor dry-run
            from ..editing.protocol import parse_request
            from ..editing.executor import EditExecutor

            try:
                req = parse_request(edits)
            except Exception as e:
                err_msg = f"åè®®æ ¡éªŒå¤±è´¥: {e}"
                print("\n" + "="*80)
                print("âŒ ç¼–è¾‘æŒ‡ä»¤éªŒè¯å¤±è´¥")
                print("="*80)
                print(f"é”™è¯¯: {err_msg}")
                print(f"\nğŸ“‹ æ¨¡å‹è¾“å‡º (å‰ 1500 å­—ç¬¦):")
                print(json.dumps(edits, ensure_ascii=False)[:1500])
                print("="*80 + "\n")
                ctx.events.emit("patch.verify.fail", {"error": err_msg})
                last_error = err_msg
                if attempt < max_retries:
                    prompt_msgs.append(ChatMessage(
                        role="user",
                        content=f"{err_msg}ã€‚è¯·æŒ‰ç¼–è¾‘åè®®è¾“å‡º JSONï¼Œå¹¶ç¡®ä¿å­—æ®µé½å…¨ã€‚",
                    ))
                    attempt += 1
                    continue
                else:
                    ctx.events.emit("patch.apply.final_fail", {"error": err_msg})
                    return AgentResult(status="skip", outputs={"notes": [err_msg]})

            executor = EditExecutor(ctx.file_contents, Path(ctx.workspace))
            dry = executor.apply(req, dry_run=True)
            if dry.ok:
                ctx.events.emit("patch.verify.success", {"edit_count": len(req.edits)})
                final_edits = json.dumps(edits, ensure_ascii=False, indent=2)
                break
            else:
                err_msg = dry.error or "éªŒè¯å¤±è´¥"
                print("\n" + "="*80)
                print("âŒ ç¼–è¾‘æŒ‡ä»¤éªŒè¯å¤±è´¥")
                print("="*80)
                print(f"é”™è¯¯: {err_msg}")
                print(f"\nğŸ“‹ ç”Ÿæˆçš„ç¼–è¾‘æŒ‡ä»¤ JSON (å‰ 1500 å­—ç¬¦):")
                print(json.dumps(edits, ensure_ascii=False)[:1500])
                print("="*80 + "\n")
                ctx.events.emit("patch.verify.fail", {"error": err_msg})
                last_error = err_msg
                if attempt < max_retries:
                    prompt_msgs.append(ChatMessage(
                        role="user",
                        content=f"éªŒè¯å¤±è´¥ï¼š{err_msg}ã€‚ä»…ä¿®æ­£ old_string æˆ– expected_replacementsï¼Œå†è¾“å‡º JSONã€‚",
                    ))
                    attempt += 1
                else:
                    ctx.events.emit("patch.apply.final_fail", {"error": err_msg})
                    return AgentResult(status="skip", outputs={"notes": [f"ç¼–è¾‘æŒ‡ä»¤æ— æ³•åº”ç”¨: {err_msg}"]})

        # Save edits to JSON file
        edit_path = ctx.run_manager.save_patch(ctx, 1, json.dumps(final_edits, indent=2, ensure_ascii=False))
        ctx.patch_queue.append(str(edit_path))
        
        ctx.events.emit("patch.proposed", {"count": len(final_edits), "artifacts": [str(edit_path)]})
        return AgentResult(status="ok", artifacts=[str(edit_path)], outputs={"edits": final_edits})

    def _build_prompt(self, ctx, allowed_files: list[str]) -> list[ChatMessage]:
        # 1) ä¸¥æ ¼çš„ System Promptï¼Œç¦æ­¢ markdown ä»£ç å—ï¼Œå¼ºè°ƒç²¾ç¡®åŒ¹é…ä¸é”šç‚¹
        system = (
            "You are an Automated Code Refactoring Engine. You are NOT a chat assistant.\n"
            "Your task is to output a strict JSON array containing Search & Replace operations.\n\n"
            "### CRITICAL RULES\n"
            "1. **NO MARKDOWN**: Output RAW JSON only. Do NOT use ```json or ``` tags.\n"
            "2. **EXACT MATCH**: `search_block` must be a byte-for-byte copy from the source file "
            "(preserving all spaces, indents, and newlines). Do NOT reformat or beautify code.\n"
            "3. **UNIQUENESS**: Ensure `search_block` is unique in the file. Include more context lines if needed.\n"
            "4. **ANCHORING**: To add new code, `search_block` should anchor around stable context (e.g., "
            "the previous function's closing brace) so replacement can be applied deterministically.\n\n"
            "### JSON Schema\n"
            "[\n"
            "  {\n"
            '    "file_path": "path/to/file",\n'
            '    "search_block": "exact original code content",\n'
            '    "replace_block": "new code content"\n'
            "  }\n"
            "]\n"
        )

        # 2) æ„é€ å¸¦è¾¹ç•Œçš„æ–‡ä»¶ä¸Šä¸‹æ–‡ï¼Œç¡®ä¿ search_block æ¥æºæ˜ç¡®
        file_contents_map = getattr(ctx, "file_contents", {}) or {}
        sections = []
        for f_path in allowed_files:
            content = file_contents_map.get(f_path)
            if content is None:
                content = self._read_file_content(ctx, f_path)
            sections.append(f"--- FILE: {f_path} ---\n{content}\n--- END OF {f_path} ---")
        file_context_str = "\n\n".join(sections)

        # 3) User Messageï¼Œç»™å‡ºä»»åŠ¡ä¸æ–‡ä»¶å†…å®¹
        user = (
            f"Task: {ctx.task}\n\n"
            "Based on the following file contents, generate the JSON array for Search & Replace.\n"
            "Remember: no markdown fences, raw JSON only, and search_block must be exact copies from the files.\n\n"
            f"{file_context_str}\n\n"
            "Output the JSON array now:"
        )

        return [
            ChatMessage(role="system", content=system),
            ChatMessage(role="user", content=user),
        ]

    def _read_file_content(self, ctx, file_path: str) -> str:
        repo_root = Path(getattr(ctx.tool_router, "repo_root", "."))
        full_path = repo_root / file_path
        if not full_path.exists():
            return "[File not found]"
        try:
            with open(full_path, "r", encoding="utf-8", errors="replace") as f:
                lines = f.readlines()
                if len(lines) <= 300:
                    content = "".join(lines)
                else:
                    content = "".join(lines[:150]) + "\n... (omitted middle lines) ...\n" + "".join(lines[-150:])
                # cache for later validation/execution
                if hasattr(ctx, "file_contents"):
                    ctx.file_contents[file_path] = content if len(lines) <= 300 else full_path.read_text(encoding="utf-8", errors="replace")
                return content
        except Exception as e:
            return f"[Error reading file: {e}]"

    def _parse_edits(self, text: str) -> tuple[dict, str]:
        """Parse JSON payload from LLM response, handling markdown code blocks."""
        raw = text or ""
        text = raw.strip()

        # Remove markdown code fences anywhere in the text
        if "```" in text:
            stripped = []
            for line in text.split("\n"):
                if line.strip().startswith("```"):
                    continue
                stripped.append(line)
            text = "\n".join(stripped).strip()

        try:
            payload = json.loads(text)
            if not isinstance(payload, (dict, list)):
                return None, "è¾“å‡ºå¿…é¡»æ˜¯ JSON å¯¹è±¡æˆ–æ•°ç»„"
            return payload, None
        except json.JSONDecodeError as e:
            return None, f"JSON è§£æå¤±è´¥: {e}"

    def _validate_edits(self, ctx, edits: list[dict], allowed_files: list[str]) -> tuple[bool, str]:
        """Validate that all edits can be applied."""
        repo_root = Path(getattr(ctx.tool_router, "repo_root", "."))
        allowed_set = set(allowed_files)
        
        for i, edit in enumerate(edits):
            file_path = edit["file_path"]
            search_block = edit["search_block"]
            
            # Check if file is allowed
            if file_path not in allowed_set:
                return False, f"æ–‡ä»¶ {file_path} ä¸åœ¨ ALLOWED_FILES ä¸­"
            
            # Check if file exists
            full_path = repo_root / file_path
            if not full_path.exists():
                return False, f"æ–‡ä»¶ {file_path} ä¸å­˜åœ¨"
            
            # Check if search_block exists in file
            try:
                with open(full_path, "r", encoding="utf-8", errors="replace") as f:
                    content = f.read()
                
                if search_block not in content:
                    return False, f"åœ¨æ–‡ä»¶ {file_path} ä¸­æ‰¾ä¸åˆ° search_blockï¼ˆç¬¬ {i+1} ä¸ªç¼–è¾‘ï¼‰"
                
                # Check if search_block is unique
                if content.count(search_block) > 1:
                    return False, f"åœ¨æ–‡ä»¶ {file_path} ä¸­ search_block å‡ºç°å¤šæ¬¡ï¼ˆç¬¬ {i+1} ä¸ªç¼–è¾‘ï¼‰ï¼Œæ— æ³•ç¡®å®šæ›¿æ¢ä½ç½®"
                    
            except Exception as e:
                return False, f"è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}"
        
        return True, ""

    def _collect_allowed_files(self, ctx) -> list[str]:
        files = []
        if ctx.context_pack:
            for item in ctx.context_pack.get("files", []):
                if isinstance(item, dict):
                    for m in item.get("matches", []):
                        path = m.split(":", 1)[0]
                        if path and path not in files:
                            files.append(path)
        if not files:
            files = [
                "demo_c_project/include/calculator.h",
                "demo_c_project/src/calculator.c",
                "demo_c_project/tests/test_calculator.cpp",
            ]
        return files[:10]

    def _extract_files_from_context(self, ctx) -> list:
        files = []
        if ctx.context_pack:
            for item in ctx.context_pack.get("files", []):
                if isinstance(item, dict):
                    matches = item.get("matches", [])
                    if matches:
                        files.append(matches[0].split(":")[0])
        return files[:10]
